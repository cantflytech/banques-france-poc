{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Régions détectées: 39\n",
      "  - https://www.credit-agricole.fr/professionnel/agence/alpes-provence.html (FIN DE CONNEXION) → 110 villes\n",
      "    • Aix En Provence 13090.html : 13 agences (nouveaux en base: 13)\n",
      "    • Allauch 13190.html : 1 agences (nouveaux en base: 1)\n",
      "    • Apt 84400.html : 1 agences (nouveaux en base: 1)\n",
      "    • Arles 13200.html : 4 agences (nouveaux en base: 4)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 464\u001b[39m\n\u001b[32m    460\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Terminé. Nouvelles agences insérées: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_new\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[43mscrape_credit_agricole\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 434\u001b[39m, in \u001b[36mscrape_credit_agricole\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     total_found, new_cnt, city_name = \u001b[43mparse_city_page\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_agencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    ! Timeout ville: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 341\u001b[39m, in \u001b[36mparse_city_page\u001b[39m\u001b[34m(driver, city_url, region_url, region_name, seen_agencies, cp)\u001b[39m\n\u001b[32m    339\u001b[39m driver.get(city_url)\n\u001b[32m    340\u001b[39m WebDriverWait(driver, WAIT).until(EC.presence_of_element_located((By.ID, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[43maccept_cookies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m time.sleep(\u001b[32m0.3\u001b[39m)\n\u001b[32m    343\u001b[39m html = driver.page_source\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36maccept_cookies\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m by, sel \u001b[38;5;129;01min\u001b[39;00m selectors:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m         btn = \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[43m.\u001b[49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m         click_js(driver, btn)\n\u001b[32m     68\u001b[39m         time.sleep(\u001b[32m0.4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iroum\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:137\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time.monotonic() > end_time:\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "BASE_LIST_URL    = \"https://www.credit-agricole.fr/professionnel/agence.html\"\n",
    "CHECKPOINT_FILE  = \"agence_ca/ca_checkpoint.json\"\n",
    "CITY_COUNT_FILE  = \"agence_ca/credit_agricole_city_counts.csv\"\n",
    "OUTPUT_DIR       = \"agence_ca/regions\"  # <-- un fichier par région ici\n",
    "BRAND            = \"Crédit Agricole\"\n",
    "\n",
    "PAGELOAD_TIMEOUT = 60\n",
    "WAIT             = 12\n",
    "HEADLESS         = False  # True pour headless\n",
    "# ============================================\n",
    "\n",
    "\n",
    "# ---------- Driver ----------\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if HEADLESS:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1400,1000\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_page_load_timeout(PAGELOAD_TIMEOUT)\n",
    "    return driver\n",
    "\n",
    "def click_js(driver, element):\n",
    "    driver.execute_script(\"arguments[0].click();\", element)\n",
    "\n",
    "\n",
    "# ---------- Utils ----------\n",
    "DAY_WORDS = [\"Lundi\",\"Mardi\",\"Mercredi\",\"Jeudi\",\"Vendredi\",\"Samedi\",\"Dimanche\"]\n",
    "\n",
    "def clean_phone(txt: str) -> str:\n",
    "    if not txt: return \"\"\n",
    "    txt = re.sub(r\"(?i)\\bAppeler\\s*\", \"\", txt)\n",
    "    txt = re.sub(r\"[^\\d+ ]+\", \"\", txt)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", txt).strip()\n",
    "\n",
    "def looks_like_opening_line(s: str) -> bool:\n",
    "    return any(s.startswith(d) for d in DAY_WORDS)\n",
    "\n",
    "def accept_cookies(driver):\n",
    "    selectors = [\n",
    "        (By.CSS_SELECTOR, \"button[aria-label*='Accepter']\"),\n",
    "        (By.CSS_SELECTOR, \"button[title*='Accepter']\"),\n",
    "        (By.XPATH, \"//button[contains(., 'Tout accepter') or contains(., 'Accepter')]\"),\n",
    "        (By.CSS_SELECTOR, \"#tc_privacy_button_2\"),\n",
    "        (By.CSS_SELECTOR, \"button#didomi-notice-agree-button\"),\n",
    "    ]\n",
    "    for by, sel in selectors:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((by, sel)))\n",
    "            click_js(driver, btn)\n",
    "            time.sleep(0.4)\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if \"done_agency_urls\" not in data:\n",
    "                data[\"done_agency_urls\"] = []\n",
    "            return data\n",
    "    return {\"done_city_urls\": [], \"done_agency_urls\": []}\n",
    "\n",
    "def save_checkpoint(cp):\n",
    "    os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cp, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def to_fr_decimal(x: str) -> str:\n",
    "    if not x: return \"\"\n",
    "    return str(x).replace(\".\", \",\")\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^\\w\\- ]+\", \"\", s, flags=re.U)\n",
    "    s = s.replace(\" \", \"-\")\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s or \"region\"\n",
    "\n",
    "def append_city_count(row_dict):\n",
    "    ensure_dir(os.path.dirname(CITY_COUNT_FILE))\n",
    "    df = pd.DataFrame([row_dict])\n",
    "    if not os.path.exists(CITY_COUNT_FILE):\n",
    "        df.to_csv(CITY_COUNT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "    else:\n",
    "        df.to_csv(CITY_COUNT_FILE, mode=\"a\", header=False, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "# ---------- Écriture \"un fichier par région\" ----------\n",
    "def region_output_path(region_name: str) -> str:\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "    filename = f\"credit_agricole_{slugify(region_name)}.csv\"\n",
    "    return os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "def append_one_row_fr_region(row_dict, region_name: str):\n",
    "    \"\"\"\n",
    "    Sauvegarde immédiate d'UNE agence dans le CSV de la région.\n",
    "    Colonnes: nom;adresse;code_postal;latitude;longitude;region_source\n",
    "    \"\"\"\n",
    "    path = region_output_path(region_name)\n",
    "    df = pd.DataFrame([{\n",
    "        \"nom\": row_dict.get(\"nom\",\"\"),\n",
    "        \"adresse\": row_dict.get(\"adresse\",\"\"),\n",
    "        \"code_postal\": row_dict.get(\"code_postal\",\"\"),\n",
    "        \"latitude\": to_fr_decimal(row_dict.get(\"latitude\",\"\")),\n",
    "        \"longitude\": to_fr_decimal(row_dict.get(\"longitude\",\"\")),\n",
    "        \"region_source\": f\"{BRAND} - {region_name}\",\n",
    "    }])\n",
    "    newfile = not os.path.exists(path)\n",
    "    df.to_csv(path, mode=\"a\", header=newfile, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "# ---------- Navigation (régions / villes) ----------\n",
    "def get_region_links(driver):\n",
    "    links = set()\n",
    "    for a in driver.find_elements(By.CSS_SELECTOR, \"div.indexCR-Content ul > li > a\"):\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href and \"/agence/\" in href:\n",
    "            links.add(href)\n",
    "    if not links:\n",
    "        nodes = driver.find_elements(By.XPATH, \"//*[@id='content']//div[contains(@class,'indexCR-Content')]//ul/li/a\")\n",
    "        for n in nodes:\n",
    "            href = n.get_attribute(\"href\")\n",
    "            if href and \"/agence/\" in href:\n",
    "                links.add(href)\n",
    "    return list(sorted(links))\n",
    "\n",
    "def extract_region_name(driver, region_url) -> str:\n",
    "    def slug_to_name(u: str) -> str:\n",
    "        slug = u.rstrip(\"/\").split(\"/\")[-1].split(\".\")[0]  # ex. alpes-provence\n",
    "        return \" \".join(p.capitalize() for p in slug.split(\"-\"))\n",
    "\n",
    "    try:\n",
    "        driver.get(region_url)\n",
    "        WebDriverWait(driver, WAIT).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "        accept_cookies(driver)\n",
    "        time.sleep(0.2)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        h = soup.find([\"h1\",\"h2\"])\n",
    "        if h:\n",
    "            t = h.get_text(\" \", strip=True)\n",
    "            # si le H1 contient un message technique, on ignore\n",
    "            if \"FIN DE CONNEXION\" in t.upper():\n",
    "                return slug_to_name(region_url)\n",
    "            m = re.search(r\"Nos agences.*?Crédit Agricole\\s+(.+)\", t, re.I)\n",
    "            if m:\n",
    "                name = m.group(1).strip(\" .\")\n",
    "                if name and \"FIN DE CONNEXION\" not in name.upper():\n",
    "                    return name\n",
    "            # Sinon on tente “Crédit Agricole <Nom>”\n",
    "            m2 = re.search(r\"Crédit Agricole\\s+(.+)\", t, re.I)\n",
    "            if m2:\n",
    "                name = m2.group(1).strip(\" .\")\n",
    "                if name and \"FIN DE CONNEXION\" not in name.upper():\n",
    "                    return name\n",
    "        # dernier recours : slug\n",
    "        return slug_to_name(region_url)\n",
    "    except Exception:\n",
    "        return slug_to_name(region_url)\n",
    "\n",
    "\n",
    "def get_city_links_from_region(driver, region_url):\n",
    "    # la page est déjà ouverte par extract_region_name; on y reste\n",
    "    links, seen = [], set()\n",
    "    for letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        for a in driver.find_elements(By.CSS_SELECTOR, f\"#{letter} li a\"):\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href and \"/ville/\" in href and href not in seen:\n",
    "                seen.add(href)\n",
    "                links.append(href)\n",
    "    if not links:\n",
    "        for a in driver.find_elements(By.CSS_SELECTOR, \"a[href*='/ville/']\"):\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href and href not in seen:\n",
    "                seen.add(href)\n",
    "                links.append(href)\n",
    "    if not links:\n",
    "        for a in driver.find_elements(By.XPATH, \"//*[@id='content']//a[contains(@href,'/ville/')]\"):\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href and href not in seen:\n",
    "                seen.add(href)\n",
    "                links.append(href)\n",
    "    return links  # conserve l’ordre\n",
    "\n",
    "\n",
    "# ---------- Détection type de page ----------\n",
    "def is_agency_detail_page(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if not h1:\n",
    "        return False\n",
    "    t = h1.get_text(\" \", strip=True)\n",
    "    return bool(re.search(r\"\\bAgence\\b\\s+\\b(bancaire|CA|Crédit Agricole)\\b\", t, re.I))\n",
    "\n",
    "\n",
    "# ---------- Coords ----------\n",
    "def coords_from_dataval_html(li_html: str) -> tuple[str,str]:\n",
    "    lat = lng = \"\"\n",
    "    try:\n",
    "        li = BeautifulSoup(li_html, \"html.parser\").find(\"li\")\n",
    "        if not li: return \"\",\"\"\n",
    "        data_val = li.get(\"data-val\")\n",
    "        if data_val:\n",
    "            data = json.loads(data_val)\n",
    "            lat = str(data.get(\"latitude\",\"\") or data.get(\"lat\",\"\"))\n",
    "            lng = str(data.get(\"longitude\",\"\") or data.get(\"lng\",\"\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return lat, lng\n",
    "\n",
    "def coords_from_jsonld(html) -> tuple[str,str]:\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for s in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "            data = json.loads(s.string or \"{}\")\n",
    "            if isinstance(data, list):\n",
    "                for d in data:\n",
    "                    g = d.get(\"geo\", {})\n",
    "                    if \"latitude\" in g and \"longitude\" in g:\n",
    "                        return str(g[\"latitude\"]), str(g[\"longitude\"])\n",
    "            else:\n",
    "                g = data.get(\"geo\", {})\n",
    "                if \"latitude\" in g and \"longitude\" in g:\n",
    "                    return str(g[\"latitude\"]), str(g[\"longitude\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\",\"\"\n",
    "\n",
    "\n",
    "# ---------- Parsing : fiche agence ----------\n",
    "def parse_agency_detail_html(html, url, region_url, city_hint=\"\"):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Nom (H1)\n",
    "    h1 = soup.find(\"h1\")\n",
    "    nom = h1.get_text(\" \", strip=True) if h1 else \"\"\n",
    "\n",
    "    # --- ADRESSE (structure réelle CA) ---\n",
    "    # <div class=\"npc-sl-strct-infos-ctct-adresse\">\n",
    "    #   <p>Cours du 4 Septembre</p>\n",
    "    #   <p>13390 Auriol</p>\n",
    "    # </div>\n",
    "    adresse, cp = \"\", \"\"\n",
    "    addr_div = soup.select_one(\".npc-sl-strct-infos-ctct-adresse\")\n",
    "    if addr_div:\n",
    "        lines = [p.get_text(\" \", strip=True) for p in addr_div.select(\"p\") if p.get_text(strip=True)]\n",
    "        if lines:\n",
    "            if len(lines) >= 2:\n",
    "                adresse = f\"{lines[0]}, {lines[1]}\"\n",
    "            else:\n",
    "                adresse = lines[0]\n",
    "        # CP depuis les lignes de l'adresse\n",
    "        joined = \" \".join(lines)\n",
    "        m = re.search(r\"\\b(\\d{5})\\b\", joined)\n",
    "        if m:\n",
    "            cp = m.group(1)\n",
    "\n",
    "    # Fallback si le bloc dédié n’existe pas (vieux templates)\n",
    "    if not adresse:\n",
    "        for p in soup.select(\"h1 ~ p\"):\n",
    "            text = p.get_text(\" \", strip=True)\n",
    "            if re.search(r\"\\b\\d{5}\\b\", text):\n",
    "                adresse = text\n",
    "                m = re.search(r\"\\b(\\d{5})\\b\", text)\n",
    "                cp = m.group(1) if m else cp\n",
    "                break\n",
    "\n",
    "    # Téléphone (href=\"tel:\")\n",
    "    tel_a = soup.find(\"a\", href=re.compile(r\"^tel:\", re.I))\n",
    "    telephone = clean_phone(tel_a.get_text(\" \", strip=True)) if tel_a else \"\"\n",
    "\n",
    "    # Email (href=\"mailto:\")\n",
    "    mail_a = soup.find(\"a\", href=re.compile(r\"^mailto:\", re.I))\n",
    "    email = re.sub(r\"(?i)^mailto:\", \"\", mail_a.get(\"href\",\"\")).strip() if mail_a else \"\"\n",
    "\n",
    "    # Statut (évite “Fermer le bandeau…”)\n",
    "    statut = \"\"\n",
    "    badge = soup.find(string=re.compile(r\"\\b(Ouvert|Fermé|Fermée)\\b\", re.I))\n",
    "    if badge:\n",
    "        statut = re.search(r\"\\b(Ouvert|Fermé|Fermée)\\b\", badge, re.I).group(1).capitalize()\n",
    "\n",
    "    # Horaires : on garde que les lignes qui commencent par un jour\n",
    "    horaires = \"\"\n",
    "    title = soup.find(string=re.compile(r\"Horaires d'ouverture\", re.I))\n",
    "    if title:\n",
    "        parent = getattr(title, \"parent\", None)\n",
    "        container = parent.find_next_sibling() if parent else None\n",
    "        if container:\n",
    "            lines = [s.strip() for s in container.get_text(\"\\n\", strip=True).splitlines()]\n",
    "            lines = [s for s in lines if s and looks_like_opening_line(s)]\n",
    "            if lines:\n",
    "                horaires = \"\\n\".join(lines)\n",
    "\n",
    "    return {\n",
    "        \"nom\": nom,\n",
    "        \"adresse\": adresse,\n",
    "        \"code_postal\": cp,\n",
    "        \"telephone\": telephone,\n",
    "        \"email\": email,\n",
    "        \"statut\": statut,\n",
    "        \"horaires\": horaires,\n",
    "        \"latitude\": \"\",\n",
    "        \"longitude\": \"\",\n",
    "        \"region_url\": region_url,\n",
    "        \"city_url\": \"\",\n",
    "        \"source_url\": url,\n",
    "        \"ville\": city_hint,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Parsing : page ville ----------\n",
    "def wait_for_agency_cards(driver):\n",
    "    deadline = time.time() + WAIT\n",
    "    while time.time() < deadline:\n",
    "        items = driver.find_elements(By.CSS_SELECTOR, \"li.js-storeLoc-agency[data-val]\")\n",
    "        if items:\n",
    "            return True\n",
    "        driver.execute_script(\"window.scrollBy(0, 800);\")\n",
    "        time.sleep(0.25)\n",
    "    return False\n",
    "\n",
    "def extract_city_name_from_city_html(html, fallback_url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    h = soup.find([\"h1\",\"h2\"])\n",
    "    if h:\n",
    "        t = h.get_text(\" \", strip=True)\n",
    "        m = re.search(r\"Nos agences à\\s+(.+)\", t, re.I)\n",
    "        if m:\n",
    "            name = m.group(1).strip(\" .\")\n",
    "            if name and \"FIN DE CONNEXION\" not in name.upper():\n",
    "                return name\n",
    "        if t and \"FIN DE CONNEXION\" not in t.upper():\n",
    "            return t.strip()\n",
    "    slug = fallback_url.rstrip(\"/\").split(\"/\")[-1].split(\"?\")[0]\n",
    "    parts = slug.split(\"-\")\n",
    "    if parts and re.fullmatch(r\"\\d{5}\", parts[-1]):\n",
    "        parts = parts[:-1]\n",
    "    return \" \".join(p.capitalize() for p in parts)\n",
    "\n",
    "def collect_agency_links_from_city(driver):\n",
    "    out, seen = [], set()\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"li.js-storeLoc-agency[data-val]\")\n",
    "    for it in items:\n",
    "        try:\n",
    "            a = it.find_element(By.CSS_SELECTOR, \"a[href$='.html']\")\n",
    "            href = a.get_attribute(\"href\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not href or href in seen or \"/agence/\" not in href:\n",
    "            continue\n",
    "        seen.add(href)\n",
    "        lat, lng = coords_from_dataval_html(it.get_attribute(\"outerHTML\"))\n",
    "        out.append({\"href\": href, \"lat\": lat, \"lng\": lng})\n",
    "    return out\n",
    "\n",
    "def parse_city_page(driver, city_url, region_url, region_name, seen_agencies, cp):\n",
    "    \"\"\"\n",
    "    Sauvegarde ligne par ligne (dans le fichier de la région).\n",
    "    Retourne (total_trouves, total_nouveaux, city_name)\n",
    "    \"\"\"\n",
    "    driver.get(city_url)\n",
    "    WebDriverWait(driver, WAIT).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "    accept_cookies(driver)\n",
    "    time.sleep(0.3)\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Cas 1 : URL déjà sur une fiche\n",
    "    if is_agency_detail_page(html):\n",
    "        city_name = extract_city_name_from_city_html(html, city_url)\n",
    "        agency_url = city_url\n",
    "        total_found = 1\n",
    "        new_cnt = 0\n",
    "\n",
    "        if agency_url not in seen_agencies:\n",
    "            ag = parse_agency_detail_html(html, agency_url, region_url, city_hint=city_name)\n",
    "            lat, lng = coords_from_jsonld(html)\n",
    "            ag[\"latitude\"], ag[\"longitude\"] = lat, lng\n",
    "            ag[\"city_url\"] = city_url\n",
    "\n",
    "            append_one_row_fr_region(ag, region_name)\n",
    "            seen_agencies.add(agency_url)\n",
    "            cp[\"done_agency_urls\"].append(agency_url)\n",
    "            save_checkpoint(cp)\n",
    "            new_cnt = 1\n",
    "\n",
    "        return total_found, new_cnt, city_name\n",
    "\n",
    "    # Cas 2 : vraie page ville → lister les fiches\n",
    "    wait_for_agency_cards(driver)\n",
    "    html = driver.page_source\n",
    "    city_name = extract_city_name_from_city_html(html, city_url)\n",
    "\n",
    "    detail_links = collect_agency_links_from_city(driver)\n",
    "    total_found = len(detail_links)\n",
    "    new_cnt = 0\n",
    "\n",
    "    for obj in detail_links:\n",
    "        href, lat_hint, lng_hint = obj[\"href\"], obj[\"lat\"], obj[\"lng\"]\n",
    "        if href in seen_agencies:\n",
    "            continue\n",
    "        try:\n",
    "            driver.get(href)\n",
    "            WebDriverWait(driver, WAIT).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"h1\")))\n",
    "            time.sleep(0.2)\n",
    "            detail_html = driver.page_source\n",
    "            ag = parse_agency_detail_html(detail_html, href, region_url, city_hint=city_name)\n",
    "            # coords : priorité data-val, sinon JSON-LD\n",
    "            lat, lng = lat_hint, lng_hint\n",
    "            if not lat or not lng:\n",
    "                lat2, lng2 = coords_from_jsonld(detail_html)\n",
    "                lat = lat or lat2\n",
    "                lng = lng or lng2\n",
    "            ag[\"latitude\"], ag[\"longitude\"] = lat, lng\n",
    "            ag[\"city_url\"] = city_url\n",
    "\n",
    "            append_one_row_fr_region(ag, region_name)  # <<< un fichier par région\n",
    "            seen_agencies.add(href)\n",
    "            cp[\"done_agency_urls\"].append(href)\n",
    "            save_checkpoint(cp)\n",
    "            new_cnt += 1\n",
    "\n",
    "            driver.back()\n",
    "            WebDriverWait(driver, WAIT).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "\n",
    "    return total_found, new_cnt, city_name\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def scrape_credit_agricole():\n",
    "    driver = setup_driver()\n",
    "    cp = load_checkpoint()\n",
    "    done_cities   = set(cp.get(\"done_city_urls\", []))\n",
    "    done_agencies = set(cp.get(\"done_agency_urls\", []))\n",
    "\n",
    "    # 1) Page liste régions\n",
    "    driver.get(BASE_LIST_URL)\n",
    "    WebDriverWait(driver, WAIT).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "    accept_cookies(driver)\n",
    "    region_links = get_region_links(driver)\n",
    "    print(f\"[INFO] Régions détectées: {len(region_links)}\")\n",
    "\n",
    "    total_new = 0\n",
    "    for r_url in region_links:\n",
    "        # on ouvre la région, lit le nom affiché, et on reste dessus\n",
    "        region_name = extract_region_name(driver, r_url)\n",
    "        city_links = get_city_links_from_region(driver, r_url)\n",
    "        print(f\"  - {r_url} ({region_name}) → {len(city_links)} villes\")\n",
    "\n",
    "        region_new_sum = 0\n",
    "        for c_url in city_links:\n",
    "            if c_url in done_cities:\n",
    "                continue\n",
    "            try:\n",
    "                total_found, new_cnt, city_name = parse_city_page(\n",
    "                    driver, c_url, r_url, region_name, done_agencies, cp\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"    ! Timeout ville: {c_url}\")\n",
    "                continue\n",
    "\n",
    "            total_new += new_cnt\n",
    "            region_new_sum += new_cnt\n",
    "\n",
    "            append_city_count({\n",
    "                \"region_url\": r_url,\n",
    "                \"ville_url\": c_url,\n",
    "                \"ville\": city_name if city_name else \"\",\n",
    "                \"agences_trouvees_page\": total_found,\n",
    "                \"agences_nouvelles_sauvegardees\": new_cnt\n",
    "            })\n",
    "            print(f\"    • {city_name} : {total_found} agences (nouveaux en base: {new_cnt})\")\n",
    "\n",
    "            done_cities.add(c_url)\n",
    "            cp[\"done_city_urls\"] = list(done_cities)\n",
    "            save_checkpoint(cp)\n",
    "\n",
    "        print(f\"  = Nouveaux en base sur la région « {region_name} » : {region_new_sum}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\n✅ Terminé. Nouvelles agences insérées: {total_new}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_credit_agricole()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
